{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4424, 29)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np # linear algebra\n",
    "\n",
    "# Replace 'path_to_dataset' with the actual path to your dataset file\n",
    "df = pd.read_excel('Dataset2neww.xlsx')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Application mode', 'Application order', 'Daytime/evening attendance\\t',\n",
    "       'Previous qualification', 'Admission grade', 'Displaced', 'Gender',\n",
    "       'Age at enrollment', 'Curricular units 1st sem (evaluations)',\n",
    "       'Curricular units 1st sem (without evaluations)',\n",
    "       'Curricular units 2nd sem (credited)',\n",
    "       'Curricular units 2nd sem (evaluations)',\n",
    "       'Curricular units 2nd sem (grade)',\n",
    "       'Curricular units 2nd sem (without evaluations)', 'Unemployment rate',\n",
    "       'Inflation rate', 'GDP', 'Target']]\n",
    "y = df['Course']\n",
    "\n",
    "categorical_columns = X.select_dtypes(include=['object']).columns\n",
    "numerical_columns = X.select_dtypes(include=['int', 'float']).columns\n",
    "\n",
    "# Create DataFrames for categorical and numerical columns\n",
    "X_categorical = X[categorical_columns]\n",
    "X_numerical = X[numerical_columns]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "# Perform label encoding only on columns with dtype object\n",
    "label_encoder = LabelEncoder()\n",
    "X_encoded = X_categorical.copy()\n",
    "for col in categorical_columns:\n",
    "    X_encoded[col] = X[col].astype(str)  # Ensure the column is of string dtype before label encoding\n",
    "    X_encoded[col] = label_encoder.fit_transform(X_encoded[col])\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "# Initialize LabelEncoder for the target variable y\n",
    "label_encoder_y = LabelEncoder()\n",
    "y_encoded = label_encoder_y.fit_transform(y)\n",
    "# Merge with original X containing int and float columns\n",
    "X_final = pd.concat([X_encoded, X.select_dtypes(include=['int', 'float'])], axis=1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_encoded, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled_smote, y_train_resampled_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "borderline_smote = BorderlineSMOTE(random_state=42)\n",
    "X_train_resampled_borderline, y_train_resampled_borderline = borderline_smote.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BorutaShap SMOTE + Bordeline SMOTE\n",
    "focus on Bordeline SMOTE only also can"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best Parameters: {'max_depth': None, 'n_estimators': 200}\n",
      "Accuracy (Best Random Forest with SMOTE): 0.5084745762711864\n",
      "Classification Report (Best Random Forest with SMOTE):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.125     0.200     0.154         5\n",
      "           1      0.630     1.000     0.773        34\n",
      "           2      0.735     0.781     0.758        32\n",
      "           3      0.487     0.780     0.600        50\n",
      "           4      0.472     0.436     0.453        39\n",
      "           5      0.433     0.382     0.406        68\n",
      "           6      0.343     0.697     0.460        33\n",
      "           7      0.438     0.179     0.255        39\n",
      "           8      0.385     0.312     0.345        64\n",
      "           9      0.487     0.507     0.497        75\n",
      "          10      0.233     0.189     0.208        53\n",
      "          11      0.720     0.712     0.716       163\n",
      "          12      0.429     0.391     0.409        23\n",
      "          13      0.256     0.250     0.253        40\n",
      "          14      0.466     0.346     0.397        78\n",
      "          15      0.281     0.220     0.247        41\n",
      "          16      0.848     0.812     0.830        48\n",
      "\n",
      "    accuracy                          0.508       885\n",
      "   macro avg      0.457     0.482     0.456       885\n",
      "weighted avg      0.504     0.508     0.496       885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "rf_classifier.fit(X_train_resampled_smote, y_train_resampled_smote)\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5, None],\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_classifier,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train_resampled_smote, y_train_resampled_smote)\n",
    "\n",
    "# Get the best model\n",
    "best_rf_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred_best_rf = best_rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the best model\n",
    "accuracy_best_rf = accuracy_score(y_test, y_pred_best_rf)\n",
    "classification_report_result_best_rf = classification_report(y_test, y_pred_best_rf, digits=3)\n",
    "\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Accuracy (Best Random Forest with SMOTE): {accuracy_best_rf}\")\n",
    "print(\"Classification Report (Best Random Forest with SMOTE):\\n\", classification_report_result_best_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best Parameters: {'max_depth': None, 'n_estimators': 200}\n",
      "Accuracy (Best Random Forest with Borderline SMOTE): 0.5084745762711864\n",
      "Classification Report (Best Random Forest with Borderline SMOTE):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000         5\n",
      "           1      0.567     1.000     0.723        34\n",
      "           2      0.758     0.781     0.769        32\n",
      "           3      0.430     0.740     0.544        50\n",
      "           4      0.462     0.462     0.462        39\n",
      "           5      0.508     0.456     0.481        68\n",
      "           6      0.333     0.545     0.414        33\n",
      "           7      0.190     0.103     0.133        39\n",
      "           8      0.424     0.438     0.431        64\n",
      "           9      0.447     0.453     0.450        75\n",
      "          10      0.216     0.151     0.178        53\n",
      "          11      0.686     0.736     0.710       163\n",
      "          12      0.400     0.174     0.242        23\n",
      "          13      0.306     0.275     0.289        40\n",
      "          14      0.529     0.346     0.419        78\n",
      "          15      0.333     0.268     0.297        41\n",
      "          16      0.851     0.833     0.842        48\n",
      "\n",
      "    accuracy                          0.508       885\n",
      "   macro avg      0.438     0.457     0.434       885\n",
      "weighted avg      0.494     0.508     0.491       885\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\python\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\python\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize Borderline SMOTE\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5, None],\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_classifier,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV with resampled data\n",
    "grid_search.fit(X_train_resampled_borderline, y_train_resampled_borderline)\n",
    "\n",
    "# Get the best model\n",
    "best_rf_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred_best_rf = best_rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the best model\n",
    "accuracy_best_rf = accuracy_score(y_test, y_pred_best_rf)\n",
    "classification_report_result_best_rf = classification_report(y_test, y_pred_best_rf, digits=3)\n",
    "\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Accuracy (Best Random Forest with Borderline SMOTE): {accuracy_best_rf}\")\n",
    "print(\"Classification Report (Best Random Forest with Borderline SMOTE):\\n\", classification_report_result_best_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best Parameters: {'var_smoothing': 3.162e-05}\n",
      "Accuracy (Best GaussianNB with SMOTE): 0.25649717514124293\n",
      "Classification Report (Best GaussianNB with SMOTE):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.027     0.200     0.048         5\n",
      "           1      0.314     0.647     0.423        34\n",
      "           2      0.692     0.562     0.621        32\n",
      "           3      0.303     0.740     0.430        50\n",
      "           4      1.000     0.026     0.050        39\n",
      "           5      0.222     0.029     0.052        68\n",
      "           6      0.250     0.273     0.261        33\n",
      "           7      0.364     0.205     0.262        39\n",
      "           8      0.333     0.016     0.030        64\n",
      "           9      0.188     0.040     0.066        75\n",
      "          10      0.333     0.019     0.036        53\n",
      "          11      0.385     0.092     0.149       163\n",
      "          12      0.062     0.043     0.051        23\n",
      "          13      0.182     0.050     0.078        40\n",
      "          14      0.147     0.641     0.239        78\n",
      "          15      0.200     0.390     0.264        41\n",
      "          16      0.741     0.833     0.784        48\n",
      "\n",
      "    accuracy                          0.256       885\n",
      "   macro avg      0.338     0.283     0.226       885\n",
      "weighted avg      0.344     0.256     0.209       885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'var_smoothing': [1e-9, 1.778e-7, 3.162e-5, 5.6e-3, 1]\n",
    "}\n",
    "\n",
    "# Initialize the GaussianNB classifier\n",
    "gnb_classifier = GaussianNB()\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=gnb_classifier,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train_resampled_smote, y_train_resampled_smote)\n",
    "\n",
    "# Get the best model\n",
    "best_gnb_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred_best_gnb = best_gnb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the best model\n",
    "accuracy_best_gnb = accuracy_score(y_test, y_pred_best_gnb)\n",
    "classification_report_result_best_gnb = classification_report(y_test, y_pred_best_gnb, digits=3)\n",
    "\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Accuracy (Best GaussianNB with SMOTE): {accuracy_best_gnb}\")\n",
    "print(\"Classification Report (Best GaussianNB with SMOTE):\\n\", classification_report_result_best_gnb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best Parameters: {'var_smoothing': 3.162e-05}\n",
      "Accuracy (Best GaussianNB with SMOTE): 0.25875706214689265\n",
      "Classification Report (Best GaussianNB with SMOTE):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000         5\n",
      "           1      0.329     0.706     0.449        34\n",
      "           2      0.667     0.562     0.610        32\n",
      "           3      0.294     0.740     0.420        50\n",
      "           4      1.000     0.026     0.050        39\n",
      "           5      0.500     0.074     0.128        68\n",
      "           6      0.238     0.303     0.267        33\n",
      "           7      0.276     0.205     0.235        39\n",
      "           8      0.000     0.000     0.000        64\n",
      "           9      0.167     0.040     0.065        75\n",
      "          10      0.250     0.038     0.066        53\n",
      "          11      0.364     0.074     0.122       163\n",
      "          12      0.056     0.087     0.068        23\n",
      "          13      0.083     0.025     0.038        40\n",
      "          14      0.154     0.628     0.247        78\n",
      "          15      0.205     0.439     0.279        41\n",
      "          16      0.736     0.812     0.772        48\n",
      "\n",
      "    accuracy                          0.259       885\n",
      "   macro avg      0.313     0.280     0.225       885\n",
      "weighted avg      0.321     0.259     0.207       885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'var_smoothing': [1e-9, 1.778e-7, 3.162e-5, 5.6e-3, 1]\n",
    "}\n",
    "\n",
    "# Initialize the GaussianNB classifier\n",
    "gnb_classifier = GaussianNB()\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=gnb_classifier,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train_resampled_borderline, y_train_resampled_borderline)\n",
    "\n",
    "# Get the best model\n",
    "best_gnb_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred_best_gnb = best_gnb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the best model\n",
    "accuracy_best_gnb = accuracy_score(y_test, y_pred_best_gnb)\n",
    "classification_report_result_best_gnb = classification_report(y_test, y_pred_best_gnb, digits=3)\n",
    "\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Accuracy (Best GaussianNB with SMOTE): {accuracy_best_gnb}\")\n",
    "print(\"Classification Report (Best GaussianNB with SMOTE):\\n\", classification_report_result_best_gnb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best Parameters: {'max_depth': None}\n",
      "Accuracy (Best Decision Tree with SMOTE): 0.4090395480225989\n",
      "Classification Report (Best Decision Tree with SMOTE):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000         5\n",
      "           1      0.609     0.824     0.700        34\n",
      "           2      0.649     0.750     0.696        32\n",
      "           3      0.538     0.560     0.549        50\n",
      "           4      0.308     0.410     0.352        39\n",
      "           5      0.308     0.294     0.301        68\n",
      "           6      0.360     0.545     0.434        33\n",
      "           7      0.161     0.128     0.143        39\n",
      "           8      0.254     0.266     0.260        64\n",
      "           9      0.366     0.347     0.356        75\n",
      "          10      0.178     0.245     0.206        53\n",
      "          11      0.746     0.558     0.639       163\n",
      "          12      0.333     0.304     0.318        23\n",
      "          13      0.163     0.200     0.180        40\n",
      "          14      0.309     0.269     0.288        78\n",
      "          15      0.161     0.122     0.139        41\n",
      "          16      0.814     0.729     0.769        48\n",
      "\n",
      "    accuracy                          0.409       885\n",
      "   macro avg      0.368     0.385     0.372       885\n",
      "weighted avg      0.427     0.409     0.413       885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5, None]\n",
    "}\n",
    "\n",
    "# Initialize the Decision Tree classifier\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=dt_classifier,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train_resampled_smote, y_train_resampled_smote)\n",
    "\n",
    "# Get the best model\n",
    "best_dt_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred_best_dt = best_dt_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the best model\n",
    "accuracy_best_dt = accuracy_score(y_test, y_pred_best_dt)\n",
    "classification_report_result_best_dt = classification_report(y_test, y_pred_best_dt, digits=3)\n",
    "\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Accuracy (Best Decision Tree with SMOTE): {accuracy_best_dt}\")\n",
    "print(\"Classification Report (Best Decision Tree with SMOTE):\\n\", classification_report_result_best_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best Parameters: {'max_depth': None}\n",
      "Accuracy (Best Decision Tree with SMOTE): 0.3751412429378531\n",
      "Classification Report (Best Decision Tree with SMOTE):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000         5\n",
      "           1      0.634     0.765     0.693        34\n",
      "           2      0.625     0.625     0.625        32\n",
      "           3      0.407     0.440     0.423        50\n",
      "           4      0.158     0.154     0.156        39\n",
      "           5      0.301     0.324     0.312        68\n",
      "           6      0.349     0.455     0.395        33\n",
      "           7      0.138     0.103     0.118        39\n",
      "           8      0.279     0.297     0.288        64\n",
      "           9      0.299     0.267     0.282        75\n",
      "          10      0.129     0.170     0.146        53\n",
      "          11      0.672     0.564     0.613       163\n",
      "          12      0.227     0.217     0.222        23\n",
      "          13      0.167     0.225     0.191        40\n",
      "          14      0.227     0.192     0.208        78\n",
      "          15      0.293     0.293     0.293        41\n",
      "          16      0.750     0.750     0.750        48\n",
      "\n",
      "    accuracy                          0.375       885\n",
      "   macro avg      0.333     0.343     0.336       885\n",
      "weighted avg      0.384     0.375     0.377       885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5, None]\n",
    "}\n",
    "\n",
    "# Initialize the Decision Tree classifier\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=dt_classifier,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train_resampled_borderline, y_train_resampled_borderline)\n",
    "\n",
    "# Get the best model\n",
    "best_dt_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred_best_dt = best_dt_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the best model\n",
    "accuracy_best_dt = accuracy_score(y_test, y_pred_best_dt)\n",
    "classification_report_result_best_dt = classification_report(y_test, y_pred_best_dt, digits=3)\n",
    "\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Accuracy (Best Decision Tree with SMOTE): {accuracy_best_dt}\")\n",
    "print(\"Classification Report (Best Decision Tree with SMOTE):\\n\", classification_report_result_best_dt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best Parameters: {'n_neighbors': 1}\n",
      "Accuracy (Best KNN with SMOTE): 0.3887005649717514\n",
      "Classification Report (Best KNN with SMOTE):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.200     0.200     0.200         5\n",
      "           1      0.714     0.882     0.789        34\n",
      "           2      0.636     0.656     0.646        32\n",
      "           3      0.489     0.440     0.463        50\n",
      "           4      0.270     0.256     0.263        39\n",
      "           5      0.406     0.382     0.394        68\n",
      "           6      0.306     0.455     0.366        33\n",
      "           7      0.250     0.179     0.209        39\n",
      "           8      0.203     0.203     0.203        64\n",
      "           9      0.309     0.333     0.321        75\n",
      "          10      0.109     0.113     0.111        53\n",
      "          11      0.551     0.564     0.558       163\n",
      "          12      0.125     0.130     0.128        23\n",
      "          13      0.156     0.175     0.165        40\n",
      "          14      0.333     0.256     0.290        78\n",
      "          15      0.244     0.244     0.244        41\n",
      "          16      0.800     0.750     0.774        48\n",
      "\n",
      "    accuracy                          0.389       885\n",
      "   macro avg      0.359     0.366     0.360       885\n",
      "weighted avg      0.388     0.389     0.387       885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume X_final and y_encoded are your features and target after preprocessing\n",
    "\n",
    "\n",
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply SMOTE to the training data only\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_neighbors': [1, 2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "# Initialize the KNN classifier\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=knn_classifier,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV with resampled data\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get the best model\n",
    "best_knn_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred_best_knn = best_knn_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the best model\n",
    "accuracy_best_knn = accuracy_score(y_test, y_pred_best_knn)\n",
    "classification_report_result_best_knn = classification_report(y_test, y_pred_best_knn, digits=3)\n",
    "\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Accuracy (Best KNN with SMOTE): {accuracy_best_knn}\")\n",
    "print(\"Classification Report (Best KNN with SMOTE):\\n\", classification_report_result_best_knn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best Parameters: {'n_neighbors': 1}\n",
      "Accuracy (Best KNN with Borderline SMOTE): 0.3887005649717514\n",
      "Classification Report (Best KNN with Borderline SMOTE):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000         5\n",
      "           1      0.698     0.882     0.779        34\n",
      "           2      0.667     0.750     0.706        32\n",
      "           3      0.489     0.440     0.463        50\n",
      "           4      0.278     0.256     0.267        39\n",
      "           5      0.371     0.382     0.377        68\n",
      "           6      0.286     0.485     0.360        33\n",
      "           7      0.269     0.179     0.215        39\n",
      "           8      0.217     0.203     0.210        64\n",
      "           9      0.293     0.293     0.293        75\n",
      "          10      0.088     0.094     0.091        53\n",
      "          11      0.532     0.558     0.545       163\n",
      "          12      0.222     0.174     0.195        23\n",
      "          13      0.152     0.175     0.163        40\n",
      "          14      0.364     0.256     0.301        78\n",
      "          15      0.234     0.268     0.250        41\n",
      "          16      0.857     0.750     0.800        48\n",
      "\n",
      "    accuracy                          0.389       885\n",
      "   macro avg      0.354     0.362     0.354       885\n",
      "weighted avg      0.388     0.389     0.385       885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume X_final and y_encoded are your features and target after preprocessing\n",
    "\n",
    "\n",
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply Borderline SMOTE to the training data only\n",
    "smote = BorderlineSMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_neighbors': [1, 2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "# Initialize the KNN classifier\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=knn_classifier,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV with resampled data\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get the best model\n",
    "best_knn_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred_best_knn = best_knn_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the best model\n",
    "accuracy_best_knn = accuracy_score(y_test, y_pred_best_knn)\n",
    "classification_report_result_best_knn = classification_report(y_test, y_pred_best_knn, digits=3)\n",
    "\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Accuracy (Best KNN with Borderline SMOTE): {accuracy_best_knn}\")\n",
    "print(\"Classification Report (Best KNN with Borderline SMOTE):\\n\", classification_report_result_best_knn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Best Parameters (SVM with SMOTE): {'C': 100, 'gamma': 0.1}\n",
      "Accuracy (SVM with SMOTE): 0.4271186440677966\n",
      "Classification Report (SVM with SMOTE):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000         5\n",
      "           1      0.689     0.912     0.785        34\n",
      "           2      0.686     0.750     0.716        32\n",
      "           3      0.448     0.520     0.481        50\n",
      "           4      0.239     0.282     0.259        39\n",
      "           5      0.352     0.456     0.397        68\n",
      "           6      0.293     0.364     0.324        33\n",
      "           7      0.120     0.077     0.094        39\n",
      "           8      0.243     0.266     0.254        64\n",
      "           9      0.372     0.427     0.398        75\n",
      "          10      0.176     0.170     0.173        53\n",
      "          11      0.630     0.638     0.634       163\n",
      "          12      0.133     0.087     0.105        23\n",
      "          13      0.212     0.175     0.192        40\n",
      "          14      0.471     0.308     0.372        78\n",
      "          15      0.286     0.195     0.232        41\n",
      "          16      0.822     0.771     0.796        48\n",
      "\n",
      "    accuracy                          0.427       885\n",
      "   macro avg      0.363     0.376     0.365       885\n",
      "weighted avg      0.419     0.427     0.419       885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "\n",
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Define the parameter grid for SVM\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "}\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "svm_classifier = SVC()\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=svm_classifier,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get the best model\n",
    "best_svm_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred_best_svm = best_svm_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the best model\n",
    "accuracy_best_svm = accuracy_score(y_test, y_pred_best_svm)\n",
    "classification_report_result_best_svm = classification_report(y_test, y_pred_best_svm, digits=3)\n",
    "\n",
    "# Print results\n",
    "print(f\"Best Parameters (SVM with SMOTE): {grid_search.best_params_}\")\n",
    "print(f\"Accuracy (SVM with SMOTE): {accuracy_best_svm}\")\n",
    "print(\"Classification Report (SVM with SMOTE):\\n\", classification_report_result_best_svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Best Parameters (SVM with BorderlineSMOTE): {'C': 100, 'gamma': 0.1}\n",
      "Accuracy (SVM with BorderlineSMOTE): 0.43615819209039547\n",
      "Classification Report (SVM with BorderlineSMOTE):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000         5\n",
      "           1      0.705     0.912     0.795        34\n",
      "           2      0.676     0.719     0.697        32\n",
      "           3      0.441     0.520     0.477        50\n",
      "           4      0.260     0.333     0.292        39\n",
      "           5      0.341     0.426     0.379        68\n",
      "           6      0.279     0.364     0.316        33\n",
      "           7      0.208     0.128     0.159        39\n",
      "           8      0.221     0.234     0.227        64\n",
      "           9      0.407     0.493     0.446        75\n",
      "          10      0.239     0.208     0.222        53\n",
      "          11      0.659     0.663     0.661       163\n",
      "          12      0.267     0.174     0.211        23\n",
      "          13      0.179     0.175     0.177        40\n",
      "          14      0.488     0.269     0.347        78\n",
      "          15      0.212     0.171     0.189        41\n",
      "          16      0.804     0.771     0.787        48\n",
      "\n",
      "    accuracy                          0.436       885\n",
      "   macro avg      0.376     0.386     0.375       885\n",
      "weighted avg      0.431     0.436     0.428       885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are already defined\n",
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply BorderlineSMOTE\n",
    "borderline_smote = BorderlineSMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = borderline_smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Define the parameter grid for SVM\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "}\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "svm_classifier = SVC()\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=svm_classifier,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get the best model\n",
    "best_svm_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred_best_svm = best_svm_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the best model\n",
    "accuracy_best_svm = accuracy_score(y_test, y_pred_best_svm)\n",
    "classification_report_result_best_svm = classification_report(y_test, y_pred_best_svm, digits=3)\n",
    "\n",
    "# Print results\n",
    "print(f\"Best Parameters (SVM with BorderlineSMOTE): {grid_search.best_params_}\")\n",
    "print(f\"Accuracy (SVM with BorderlineSMOTE): {accuracy_best_svm}\")\n",
    "print(\"Classification Report (SVM with BorderlineSMOTE):\\n\", classification_report_result_best_svm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best Parameters (Logistic Regression with SMOTE): {'C': 100, 'solver': 'lbfgs'}\n",
      "Accuracy (Logistic Regression with SMOTE): 0.3254237288135593\n",
      "Classification Report (Logistic Regression with SMOTE):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.062     0.200     0.095         5\n",
      "           1      0.438     0.824     0.571        34\n",
      "           2      0.676     0.781     0.725        32\n",
      "           3      0.369     0.480     0.417        50\n",
      "           4      0.194     0.179     0.187        39\n",
      "           5      0.298     0.250     0.272        68\n",
      "           6      0.272     0.667     0.386        33\n",
      "           7      0.184     0.179     0.182        39\n",
      "           8      0.182     0.031     0.053        64\n",
      "           9      0.233     0.373     0.287        75\n",
      "          10      0.300     0.113     0.164        53\n",
      "          11      0.549     0.307     0.394       163\n",
      "          12      0.127     0.391     0.191        23\n",
      "          13      0.222     0.100     0.138        40\n",
      "          14      0.194     0.077     0.110        78\n",
      "          15      0.186     0.390     0.252        41\n",
      "          16      0.837     0.750     0.791        48\n",
      "\n",
      "    accuracy                          0.325       885\n",
      "   macro avg      0.313     0.358     0.307       885\n",
      "weighted avg      0.349     0.325     0.310       885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are already defined\n",
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Define the parameter grid for Logistic Regression\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'solver': ['lbfgs', 'liblinear'],  # Choose either 'lbfgs' or 'liblinear' solver\n",
    "}\n",
    "\n",
    "# Initialize the Logistic Regression classifier\n",
    "lr_classifier = LogisticRegression(max_iter=1000)  # Increase max_iter if necessary\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=lr_classifier,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get the best model\n",
    "best_lr_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred_best_lr = best_lr_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the best model\n",
    "accuracy_best_lr = accuracy_score(y_test, y_pred_best_lr)\n",
    "classification_report_result_best_lr = classification_report(y_test, y_pred_best_lr, digits=3)\n",
    "\n",
    "# Print results\n",
    "print(f\"Best Parameters (Logistic Regression with SMOTE): {grid_search.best_params_}\")\n",
    "print(f\"Accuracy (Logistic Regression with SMOTE): {accuracy_best_lr}\")\n",
    "print(\"Classification Report (Logistic Regression with SMOTE):\\n\", classification_report_result_best_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best Parameters (Logistic Regression with BorderlineSMOTE): {'C': 100, 'solver': 'lbfgs'}\n",
      "Accuracy (Logistic Regression with BorderlineSMOTE): 0.30847457627118646\n",
      "Classification Report (Logistic Regression with BorderlineSMOTE):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000         5\n",
      "           1      0.438     0.824     0.571        34\n",
      "           2      0.667     0.625     0.645        32\n",
      "           3      0.324     0.480     0.387        50\n",
      "           4      0.157     0.205     0.178        39\n",
      "           5      0.304     0.206     0.246        68\n",
      "           6      0.235     0.576     0.333        33\n",
      "           7      0.208     0.256     0.230        39\n",
      "           8      0.062     0.016     0.025        64\n",
      "           9      0.248     0.387     0.302        75\n",
      "          10      0.250     0.094     0.137        53\n",
      "          11      0.506     0.270     0.352       163\n",
      "          12      0.113     0.261     0.158        23\n",
      "          13      0.148     0.100     0.119        40\n",
      "          14      0.241     0.090     0.131        78\n",
      "          15      0.182     0.390     0.248        41\n",
      "          16      0.760     0.792     0.776        48\n",
      "\n",
      "    accuracy                          0.308       885\n",
      "   macro avg      0.285     0.328     0.285       885\n",
      "weighted avg      0.322     0.308     0.291       885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are already defined\n",
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply BorderlineSMOTE\n",
    "smote = BorderlineSMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Define the parameter grid for Logistic Regression\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'solver': ['lbfgs', 'liblinear'],  # Choose either 'lbfgs' or 'liblinear' solver\n",
    "}\n",
    "\n",
    "# Initialize the Logistic Regression classifier\n",
    "lr_classifier = LogisticRegression(max_iter=1000)  # Increase max_iter if necessary\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=lr_classifier,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get the best model\n",
    "best_lr_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred_best_lr = best_lr_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the best model\n",
    "accuracy_best_lr = accuracy_score(y_test, y_pred_best_lr)\n",
    "classification_report_result_best_lr = classification_report(y_test, y_pred_best_lr, digits=3)\n",
    "\n",
    "# Print results\n",
    "print(f\"Best Parameters (Logistic Regression with BorderlineSMOTE): {grid_search.best_params_}\")\n",
    "print(f\"Accuracy (Logistic Regression with BorderlineSMOTE): {accuracy_best_lr}\")\n",
    "print(\"Classification Report (Logistic Regression with BorderlineSMOTE):\\n\", classification_report_result_best_lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Best Parameters (XGBoost with SMOTE): {'gamma': None, 'max_depth': None}\n",
      "Accuracy (XGBoost with SMOTE): 0.5412429378531074\n",
      "Classification Report (XGBoost with SMOTE):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.125     0.200     0.154         5\n",
      "           1      0.681     0.941     0.790        34\n",
      "           2      0.719     0.719     0.719        32\n",
      "           3      0.617     0.580     0.598        50\n",
      "           4      0.455     0.513     0.482        39\n",
      "           5      0.507     0.500     0.504        68\n",
      "           6      0.391     0.545     0.456        33\n",
      "           7      0.316     0.154     0.207        39\n",
      "           8      0.412     0.438     0.424        64\n",
      "           9      0.533     0.533     0.533        75\n",
      "          10      0.234     0.208     0.220        53\n",
      "          11      0.730     0.828     0.776       163\n",
      "          12      0.421     0.348     0.381        23\n",
      "          13      0.275     0.275     0.275        40\n",
      "          14      0.542     0.410     0.467        78\n",
      "          15      0.353     0.293     0.320        41\n",
      "          16      0.812     0.812     0.812        48\n",
      "\n",
      "    accuracy                          0.541       885\n",
      "   macro avg      0.478     0.488     0.478       885\n",
      "weighted avg      0.530     0.541     0.531       885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Ignore warnings for simplicity\n",
    "\n",
    "# Define the parameter grid for XGBoost\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5, None],\n",
    "    'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, None],\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "xgb_classifier = XGBClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_classifier,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train_resampled_smote, y_train_resampled_smote)\n",
    "\n",
    "# Get the best model\n",
    "best_xgb_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred_best_xgb = best_xgb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the best model\n",
    "accuracy_best_xgb = accuracy_score(y_test, y_pred_best_xgb)\n",
    "classification_report_result_best_xgb = classification_report(y_test, y_pred_best_xgb, digits=3)\n",
    "\n",
    "# Print results\n",
    "print(f\"Best Parameters (XGBoost with SMOTE): {grid_search.best_params_}\")\n",
    "print(f\"Accuracy (XGBoost with SMOTE): {accuracy_best_xgb}\")\n",
    "print(\"Classification Report (XGBoost with SMOTE):\\n\", classification_report_result_best_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters (XGBoost with SMOTE): {'gamma': None, 'max_depth': None}\n",
      "Accuracy (XGBoost with SMOTE): 0.5435028248587571\n",
      "Classification Report (XGBoost with SMOTE):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000         5\n",
      "           1      0.653     0.941     0.771        34\n",
      "           2      0.710     0.688     0.698        32\n",
      "           3      0.576     0.680     0.624        50\n",
      "           4      0.415     0.436     0.425        39\n",
      "           5      0.559     0.559     0.559        68\n",
      "           6      0.525     0.636     0.575        33\n",
      "           7      0.316     0.154     0.207        39\n",
      "           8      0.451     0.500     0.474        64\n",
      "           9      0.444     0.480     0.462        75\n",
      "          10      0.295     0.245     0.268        53\n",
      "          11      0.721     0.810     0.763       163\n",
      "          12      0.312     0.217     0.256        23\n",
      "          13      0.375     0.300     0.333        40\n",
      "          14      0.492     0.397     0.440        78\n",
      "          15      0.289     0.268     0.278        41\n",
      "          16      0.796     0.812     0.804        48\n",
      "\n",
      "    accuracy                          0.544       885\n",
      "   macro avg      0.466     0.478     0.467       885\n",
      "weighted avg      0.524     0.544     0.529       885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Ignore warnings for simplicity\n",
    "\n",
    "# Define the parameter grid for XGBoost\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5, None],\n",
    "    'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, None],\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "xgb_classifier = XGBClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_classifier,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train_resampled_borderline, y_train_resampled_borderline)\n",
    "\n",
    "# Get the best model\n",
    "best_xgb_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred_best_xgb = best_xgb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the best model\n",
    "accuracy_best_xgb = accuracy_score(y_test, y_pred_best_xgb)\n",
    "classification_report_result_best_xgb = classification_report(y_test, y_pred_best_xgb, digits=3)\n",
    "\n",
    "# Print results\n",
    "print(f\"Best Parameters (XGBoost with SMOTE): {grid_search.best_params_}\")\n",
    "print(f\"Accuracy (XGBoost with SMOTE): {accuracy_best_xgb}\")\n",
    "print(\"Classification Report (XGBoost with SMOTE):\\n\", classification_report_result_best_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline XGBoost Model:\n",
      "Accuracy: 0.5559322033898305\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000         5\n",
      "           1      0.660     0.912     0.765        34\n",
      "           2      0.793     0.719     0.754        32\n",
      "           3      0.653     0.640     0.646        50\n",
      "           4      0.500     0.513     0.506        39\n",
      "           5      0.551     0.559     0.555        68\n",
      "           6      0.450     0.545     0.493        33\n",
      "           7      0.444     0.205     0.281        39\n",
      "           8      0.459     0.531     0.493        64\n",
      "           9      0.458     0.507     0.481        75\n",
      "          10      0.244     0.208     0.224        53\n",
      "          11      0.673     0.847     0.750       163\n",
      "          12      0.667     0.174     0.276        23\n",
      "          13      0.419     0.325     0.366        40\n",
      "          14      0.475     0.372     0.417        78\n",
      "          15      0.371     0.317     0.342        41\n",
      "          16      0.824     0.875     0.848        48\n",
      "\n",
      "    accuracy                          0.556       885\n",
      "   macro avg      0.508     0.485     0.482       885\n",
      "weighted avg      0.542     0.556     0.539       885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the XGBoost classifier with default parameters\n",
    "xgb_classifier_baseline = XGBClassifier(random_state=42)\n",
    "\n",
    "# Fit the classifier on the training data (assuming X_train and y_train are defined)\n",
    "xgb_classifier_baseline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_xgb_baseline = xgb_classifier_baseline.predict(X_test)\n",
    "\n",
    "# Evaluate the baseline model\n",
    "accuracy_xgb_baseline = accuracy_score(y_test, y_pred_xgb_baseline)\n",
    "classification_report_result_xgb_baseline = classification_report(y_test, y_pred_xgb_baseline, digits=3)\n",
    "\n",
    "# Print results\n",
    "print(\"Baseline XGBoost Model:\")\n",
    "print(f\"Accuracy: {accuracy_xgb_baseline}\")\n",
    "print(\"Classification Report:\\n\", classification_report_result_xgb_baseline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best Parameters (AdaBoost with SMOTE): {'learning_rate': 0.1, 'n_estimators': 200}\n",
      "Accuracy (AdaBoost with SMOTE): 0.28926553672316385\n",
      "Classification Report (AdaBoost with SMOTE):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000         5\n",
      "           1      0.000     0.000     0.000        34\n",
      "           2      0.568     0.781     0.658        32\n",
      "           3      0.228     0.260     0.243        50\n",
      "           4      0.133     0.103     0.116        39\n",
      "           5      0.208     0.074     0.109        68\n",
      "           6      0.148     0.636     0.240        33\n",
      "           7      0.103     0.077     0.088        39\n",
      "           8      0.000     0.000     0.000        64\n",
      "           9      0.266     0.280     0.273        75\n",
      "          10      0.000     0.000     0.000        53\n",
      "          11      0.490     0.589     0.535       163\n",
      "          12      0.096     0.304     0.146        23\n",
      "          13      0.129     0.100     0.113        40\n",
      "          14      0.381     0.103     0.162        78\n",
      "          15      0.196     0.488     0.280        41\n",
      "          16      0.806     0.604     0.690        48\n",
      "\n",
      "    accuracy                          0.289       885\n",
      "   macro avg      0.221     0.259     0.215       885\n",
      "weighted avg      0.273     0.289     0.259       885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Ignore warnings for simplicity\n",
    "\n",
    "\n",
    "# Define the parameter grid for AdaBoost\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'learning_rate': [0.001, 0.1, 1],\n",
    "}\n",
    "\n",
    "# Initialize the AdaBoost classifier\n",
    "ada_classifier = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=ada_classifier,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV on resampled data\n",
    "grid_search.fit(X_train_resampled_smote, y_train_resampled_smote)\n",
    "\n",
    "# Get the best model\n",
    "best_ada_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred_best_ada = best_ada_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the best model\n",
    "accuracy_best_ada = accuracy_score(y_test, y_pred_best_ada)\n",
    "classification_report_result_best_ada = classification_report(y_test, y_pred_best_ada, digits=3)\n",
    "\n",
    "# Print results\n",
    "print(f\"Best Parameters (AdaBoost with SMOTE): {grid_search.best_params_}\")\n",
    "print(f\"Accuracy (AdaBoost with SMOTE): {accuracy_best_ada}\")\n",
    "print(\"Classification Report (AdaBoost with SMOTE):\\n\", classification_report_result_best_ada)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best Parameters (AdaBoost with SMOTE): {'learning_rate': 0.1, 'n_estimators': 50}\n",
      "Accuracy (AdaBoost with SMOTE): 0.2576271186440678\n",
      "Classification Report (AdaBoost with SMOTE):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000         5\n",
      "           1      0.000     0.000     0.000        34\n",
      "           2      0.491     0.844     0.621        32\n",
      "           3      0.140     0.140     0.140        50\n",
      "           4      0.071     0.026     0.038        39\n",
      "           5      0.167     0.074     0.102        68\n",
      "           6      0.151     0.818     0.255        33\n",
      "           7      0.188     0.077     0.109        39\n",
      "           8      0.000     0.000     0.000        64\n",
      "           9      0.385     0.133     0.198        75\n",
      "          10      0.069     0.038     0.049        53\n",
      "          11      0.418     0.503     0.457       163\n",
      "          12      0.075     0.217     0.111        23\n",
      "          13      0.070     0.075     0.072        40\n",
      "          14      0.320     0.205     0.250        78\n",
      "          15      0.196     0.488     0.280        41\n",
      "          16      0.800     0.417     0.548        48\n",
      "\n",
      "    accuracy                          0.258       885\n",
      "   macro avg      0.208     0.238     0.190       885\n",
      "weighted avg      0.255     0.258     0.229       885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Ignore warnings for simplicity\n",
    "\n",
    "\n",
    "# Define the parameter grid for AdaBoost\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'learning_rate': [0.001, 0.1, 1],\n",
    "}\n",
    "\n",
    "# Initialize the AdaBoost classifier\n",
    "ada_classifier = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=ada_classifier,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV on resampled data\n",
    "grid_search.fit(X_train_resampled_borderline, y_train_resampled_borderline)\n",
    "\n",
    "# Get the best model\n",
    "best_ada_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred_best_ada = best_ada_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the best model\n",
    "accuracy_best_ada = accuracy_score(y_test, y_pred_best_ada)\n",
    "classification_report_result_best_ada = classification_report(y_test, y_pred_best_ada, digits=3)\n",
    "\n",
    "# Print results\n",
    "print(f\"Best Parameters (AdaBoost with SMOTE): {grid_search.best_params_}\")\n",
    "print(f\"Accuracy (AdaBoost with SMOTE): {accuracy_best_ada}\")\n",
    "print(\"Classification Report (AdaBoost with SMOTE):\\n\", classification_report_result_best_ada)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline AdaBoost Model:\n",
      "Accuracy: 0.29265536723163843\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000         5\n",
      "           1      0.420     0.853     0.563        34\n",
      "           2      0.727     0.250     0.372        32\n",
      "           3      0.000     0.000     0.000        50\n",
      "           4      0.240     0.154     0.188        39\n",
      "           5      0.111     0.044     0.063        68\n",
      "           6      0.000     0.000     0.000        33\n",
      "           7      0.000     0.000     0.000        39\n",
      "           8      0.000     0.000     0.000        64\n",
      "           9      0.274     0.227     0.248        75\n",
      "          10      0.000     0.000     0.000        53\n",
      "          11      0.246     0.926     0.389       163\n",
      "          12      0.000     0.000     0.000        23\n",
      "          13      0.000     0.000     0.000        40\n",
      "          14      0.000     0.000     0.000        78\n",
      "          15      0.000     0.000     0.000        41\n",
      "          16      0.652     0.938     0.769        48\n",
      "\n",
      "    accuracy                          0.293       885\n",
      "   macro avg      0.157     0.199     0.152       885\n",
      "weighted avg      0.166     0.293     0.183       885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the AdaBoost classifier with default parameters\n",
    "ada_classifier_baseline = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "# Fit the classifier on the training data (assuming X_train and y_train are defined)\n",
    "ada_classifier_baseline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_ada_baseline = ada_classifier_baseline.predict(X_test)\n",
    "\n",
    "# Evaluate the baseline model\n",
    "accuracy_ada_baseline = accuracy_score(y_test, y_pred_ada_baseline)\n",
    "classification_report_result_ada_baseline = classification_report(y_test, y_pred_ada_baseline, digits=3)\n",
    "\n",
    "# Print results\n",
    "print(\"Baseline AdaBoost Model:\")\n",
    "print(f\"Accuracy: {accuracy_ada_baseline}\")\n",
    "print(\"Classification Report:\\n\", classification_report_result_ada_baseline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001519 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1527\n",
      "[LightGBM] [Info] Number of data points in the train set: 10251, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -2.833213\n",
      "[LightGBM] [Info] Start training from score -2.833213\n",
      "[LightGBM] [Info] Start training from score -2.833213\n",
      "[LightGBM] [Info] Start training from score -2.833213\n",
      "[LightGBM] [Info] Start training from score -2.833213\n",
      "[LightGBM] [Info] Start training from score -2.833213\n",
      "[LightGBM] [Info] Start training from score -2.833213\n",
      "[LightGBM] [Info] Start training from score -2.833213\n",
      "[LightGBM] [Info] Start training from score -2.833213\n",
      "[LightGBM] [Info] Start training from score -2.833213\n",
      "[LightGBM] [Info] Start training from score -2.833213\n",
      "[LightGBM] [Info] Start training from score -2.833213\n",
      "[LightGBM] [Info] Start training from score -2.833213\n",
      "[LightGBM] [Info] Start training from score -2.833213\n",
      "[LightGBM] [Info] Start training from score -2.833213\n",
      "[LightGBM] [Info] Start training from score -2.833213\n",
      "[LightGBM] [Info] Start training from score -2.833213\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Best Parameters (LGBM with SMOTE): {'max_depth': -1, 'n_estimators': 200}\n",
      "Accuracy (LGBM with SMOTE): 0.5333333333333333\n",
      "Classification Report (LGBM with SMOTE):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.167     0.200     0.182         5\n",
      "           1      0.711     0.941     0.810        34\n",
      "           2      0.781     0.781     0.781        32\n",
      "           3      0.651     0.560     0.602        50\n",
      "           4      0.457     0.410     0.432        39\n",
      "           5      0.543     0.559     0.551        68\n",
      "           6      0.400     0.545     0.462        33\n",
      "           7      0.278     0.128     0.175        39\n",
      "           8      0.362     0.453     0.403        64\n",
      "           9      0.457     0.493     0.474        75\n",
      "          10      0.196     0.208     0.202        53\n",
      "          11      0.757     0.840     0.797       163\n",
      "          12      0.421     0.348     0.381        23\n",
      "          13      0.293     0.300     0.296        40\n",
      "          14      0.467     0.359     0.406        78\n",
      "          15      0.240     0.146     0.182        41\n",
      "          16      0.854     0.854     0.854        48\n",
      "\n",
      "    accuracy                          0.533       885\n",
      "   macro avg      0.473     0.478     0.470       885\n",
      "weighted avg      0.521     0.533     0.523       885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Ignore warnings for simplicity\n",
    "\n",
    "# Define the parameter grid for LGBM\n",
    "param_grid = {\n",
    "    'max_depth': [-1, 2, 3, 4, 5],\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "}\n",
    "\n",
    "# Initialize the LGBM classifier\n",
    "lgbm_classifier = LGBMClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=lgbm_classifier,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Assuming X_train_resampled_smote, y_train_resampled_smote are defined\n",
    "# Fit GridSearchCV on SMOTE resampled data\n",
    "grid_search.fit(X_train_resampled_smote, y_train_resampled_smote)\n",
    "\n",
    "# Get the best model\n",
    "best_lgbm_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred_best_lgbm = best_lgbm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the best model\n",
    "accuracy_best_lgbm = accuracy_score(y_test, y_pred_best_lgbm)\n",
    "classification_report_result_best_lgbm = classification_report(y_test, y_pred_best_lgbm, digits=3)\n",
    "\n",
    "# Print results\n",
    "print(f\"Best Parameters (LGBM with SMOTE): {grid_search.best_params_}\")\n",
    "print(f\"Accuracy (LGBM with SMOTE): {accuracy_best_lgbm}\")\n",
    "print(\"Classification Report (LGBM with SMOTE):\\n\", classification_report_result_best_lgbm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007021 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1500\n",
      "[LightGBM] [Info] Number of data points in the train set: 9655, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -7.229321\n",
      "[LightGBM] [Info] Start training from score -2.773314\n",
      "[LightGBM] [Info] Start training from score -2.773314\n",
      "[LightGBM] [Info] Start training from score -2.773314\n",
      "[LightGBM] [Info] Start training from score -2.773314\n",
      "[LightGBM] [Info] Start training from score -2.773314\n",
      "[LightGBM] [Info] Start training from score -2.773314\n",
      "[LightGBM] [Info] Start training from score -2.773314\n",
      "[LightGBM] [Info] Start training from score -2.773314\n",
      "[LightGBM] [Info] Start training from score -2.773314\n",
      "[LightGBM] [Info] Start training from score -2.773314\n",
      "[LightGBM] [Info] Start training from score -2.773314\n",
      "[LightGBM] [Info] Start training from score -2.773314\n",
      "[LightGBM] [Info] Start training from score -2.773314\n",
      "[LightGBM] [Info] Start training from score -2.773314\n",
      "[LightGBM] [Info] Start training from score -2.773314\n",
      "[LightGBM] [Info] Start training from score -2.773314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Best Parameters (LGBM with SMOTE): {'max_depth': -1, 'n_estimators': 200}\n",
      "Accuracy (LGBM with SMOTE): 0.5457627118644067\n",
      "Classification Report (LGBM with SMOTE):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000         5\n",
      "           1      0.674     0.912     0.775        34\n",
      "           2      0.710     0.688     0.698        32\n",
      "           3      0.627     0.640     0.634        50\n",
      "           4      0.486     0.436     0.459        39\n",
      "           5      0.569     0.544     0.556        68\n",
      "           6      0.429     0.545     0.480        33\n",
      "           7      0.471     0.205     0.286        39\n",
      "           8      0.393     0.516     0.446        64\n",
      "           9      0.443     0.520     0.479        75\n",
      "          10      0.277     0.245     0.260        53\n",
      "          11      0.745     0.840     0.790       163\n",
      "          12      0.467     0.304     0.368        23\n",
      "          13      0.278     0.250     0.263        40\n",
      "          14      0.544     0.397     0.459        78\n",
      "          15      0.243     0.220     0.231        41\n",
      "          16      0.796     0.812     0.804        48\n",
      "\n",
      "    accuracy                          0.546       885\n",
      "   macro avg      0.479     0.475     0.470       885\n",
      "weighted avg      0.536     0.546     0.534       885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Ignore warnings for simplicity\n",
    "\n",
    "# Define the parameter grid for LGBM\n",
    "param_grid = {\n",
    "    'max_depth': [-1, 2, 3, 4, 5],\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "}\n",
    "\n",
    "# Initialize the LGBM classifier\n",
    "lgbm_classifier = LGBMClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=lgbm_classifier,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Assuming X_train_resampled_smote, y_train_resampled_smote are defined\n",
    "# Fit GridSearchCV on SMOTE resampled data\n",
    "grid_search.fit(X_train_resampled_borderline, y_train_resampled_borderline)\n",
    "\n",
    "# Get the best model\n",
    "best_lgbm_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred_best_lgbm = best_lgbm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the best model\n",
    "accuracy_best_lgbm = accuracy_score(y_test, y_pred_best_lgbm)\n",
    "classification_report_result_best_lgbm = classification_report(y_test, y_pred_best_lgbm, digits=3)\n",
    "\n",
    "# Print results\n",
    "print(f\"Best Parameters (LGBM with SMOTE): {grid_search.best_params_}\")\n",
    "print(f\"Accuracy (LGBM with SMOTE): {accuracy_best_lgbm}\")\n",
    "print(\"Classification Report (LGBM with SMOTE):\\n\", classification_report_result_best_lgbm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
